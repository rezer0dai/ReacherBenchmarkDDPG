{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reacher ( UnityML ) DDPG benchmark\n",
    "---\n",
    "Implemented : \n",
    "  - DDPG - https://arxiv.org/abs/1509.02971\n",
    "  - n-step ( for HER only experimental ) ~ move it to GAE as in https://arxiv.org/abs/1506.02438\n",
    "  - advantage fction approximation\n",
    "  - NoisyNetworks - https://arxiv.org/abs/1706.10295\n",
    "  - SoftActor-Critic - https://arxiv.org/abs/1801.01290\n",
    "  - Prioritized Experience Replay - https://arxiv.org/abs/1511.05952\n",
    "  - postponed learning + sync\n",
    "  - Normalization - https://github.com/openai/baselines/blob/master/baselines/her/normalizer.py\n",
    "  - clipping gradients - https://pytorch.org/docs/stable/_modules/torch/nn/utils/clip_grad.html\n",
    "  \n",
    "**Disclaimer** : *portions of code borrowed from Udacity repo ( git clone https://github.com/udacity/deep-reinforcement-learning ~ ddpg_pendelum ), as well as learned some usefull quirks ( clipping gradients, hyperparams, 1:1 DDPG model as from original paper ~ except hyperparams, as well as this notebook has pretty much same skeleton and reused code )*\n",
    "\n",
    "### 1. Import the Necessary Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUBRIC\n",
    "\n",
    "DDPG is algorithm for Policy Gradients ( PG ) and especially usefull in continuous control.\n",
    "Problem with PG in general is that those require drawing from distribution, which itself is no \n",
    "issue, except bit perf slowdown, however problems is with efficiently use of experience ( replay buffer ).\n",
    "Why ? because soon samples becomes obsolete, and to estimate gradient signal we need\n",
    "to get their probabilities ( past actions probabilities by current distribution ), which\n",
    "is after several steps too off, and provide useless gradients. To counter this\n",
    "was introduced several algorithms, mainly TRPO ( which seems more like overengineering when \n",
    "confronting with AGI, as do lot of linear search in order to get more sensible actions probs for \n",
    "gradient, which is basically emulation ), on the other side PPO doing it much more simple\n",
    "by surrogating ratio of past and current probability of given action, clever math trick ( approximation ), \n",
    "and on-line sampling ( no replay buffer ). Ok, but both of those basically work in online fashion\n",
    "in terms of using only fresh experience, but forgeting about old, and as a solution to wasting\n",
    "this informations, there comes DDPG. Which is, i suppose, one of major breaktrough in PG, \n",
    "as it is simple, elegant and fast. Basically it does not play with distributions anymore, but then\n",
    "how is possible to get gradient to fix our policy ? well simple, reusing same gradients,\n",
    "as we using in our Value function, as was mathematically proven it is valid and exactly\n",
    "what we need. \n",
    "\n",
    "Besides DDPG trick, algorithm uses Soft Actor-Critic approach, which tackles problems\n",
    "with convergence, as we everytime update network and so predictions become hard to \n",
    "train and very unstable. Instead of updating our prediction network all the time,\n",
    "we will update our explorer/local network in that manner, but for prediction\n",
    "to the future we use separate target network, which is synced with explorer network\n",
    "by small portions ( like 99% of target network mix with 1% of explorer network every\n",
    "X-time we learn and update explorer ). This bring benefit also in form, that in this\n",
    "way we can use our Action-Value function (critic) to use it for approximating \n",
    "advantages ~ check Soft Actor Critic for more fancy details. And Sutton + Barto\n",
    "book for TD learning and others.\n",
    "\n",
    "In contrast to widely used epsilon-greedy or OUNoise approach for exploration,\n",
    "which is in my opinion in lot of times used as expert player ( like for MountainCar environment\n",
    "from OpenAI, OUNoise is basically pro in comparsion to pure random.. ), and\n",
    "i think it is based lot on domain knowledge of given problem, therefore far from AGI>.\n",
    "So I used NoisyNetworks, which applies Gaussian Noise directly to the weights, instead \n",
    "of to final action itself. Check paper for experiments they did. In Tennis project\n",
    "i did several updates, i will explain in following report ..\n",
    "\n",
    "To solve this environment i used DDPG + Soft Actor-Critic + NoisyNetworks, \n",
    "and learn after taking 40 steps ( which in practice means 40 steps * 20 arms as \n",
    "we have shared actor + critic ), and at that point i repeat learning\n",
    "by sampling from Prioritized Experience Buffer about 20 times ( in fact\n",
    "20 * 20 as we have more arms, so in total every 800 step i did 400 gradiend descend ).\n",
    "\n",
    "\n",
    "Man as for hyperparameters, i dont take this much seriouselly, as it is like\n",
    "finding needle in slack ... i can think of 50 different parameters you can tune,\n",
    "and whole RL is basically more alchemy than science, aka even your algorithm\n",
    "works and it is based on nice math, you change little one or two hyperparams\n",
    "and it will not converge anymore. And in addition both environments\n",
    "Tennis or Reacher, are heavilly based on *manualy* crafted reward function,\n",
    "which is in my opinion really not way to go, when we want to approach something\n",
    "close to AGI ... for later mentioned, it is nice research HER, from OpenAI,\n",
    "which tackle reward function for certain tasks ( like fetch, place, reach, .. ),\n",
    "as well as another approach for Curiosity based rewards to solve tasks like \n",
    "labyrinth etc, which i do believe is more way to go and place to invest research.\n",
    "\n",
    "But to comply with rubric, for this project and my settings ( 800 steps vs 400 learnings\n",
    "with 256 batch size ), was essential to have bigger learning rate\n",
    "for actor ( 1e-3 ) than for critic ( 1e-4 ), also i postponed sync target with \n",
    "explorer network in order of 7 gradient descents, i used bigger\n",
    "replay buffer ( 1e6 ) as checking PriorityExperienceBuffer, however\n",
    "when using Random Replay Buffer was ok to use small one ( 7e4 ).\n",
    "Also i used k-step estimates ( 3 ), and advantage instead of full value signal\n",
    "from critic, and used gradient clipping ( norm 1. )\n",
    "And as for Actor / Critic networks ( 400 x 300 params, to ensure rubrick is ok ), \n",
    "i think i blame udacity authors of this problem,\n",
    "as this should be default ( had hardcoded architecture of NN for given problem ), \n",
    "and we should, in solely opinion, focus only on RL\n",
    "part. Not to tune X-parameters with uncertaininty if our NN is capable of learning,\n",
    "man this i really dont get ... and it will consume lot of time with just\n",
    "playing with numbers, doing random search, and trying to explain loss-es,\n",
    "gradients etc in your network, instead of deepening RL knowledge ~ well \n",
    "and i dont believe explaining what our NN doing is more than random search\n",
    "as well, as understanding what really NN is capturing and how, is not one\n",
    "of feature of our understanding, is it ? ~ while i understand it is essential\n",
    "when tackling real world problem, but at course for learning RL is overkill i suppose...\n",
    "~ as in real world scenario you will more play with state representation and\n",
    "undo-over engineering of reward function i suppose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from ddpg_agent import *\n",
    "\n",
    "agent = Agent(state_size=33, action_size=4, random_seed=2, learning_delay=40, learning_repeat=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Instantiate the Environment and Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "ENV = UnityEnvironment(file_name='./data/Reacher.x86_64')\n",
    "BRAIN_NAME = ENV.brain_names[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train the Agent with DDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2\tAverage Score: 0.28             Episode 2\tAverage Score: 0.28 Recent 0.60\n",
      "Episode 4\tAverage Score: 0.48             Episode 4\tAverage Score: 0.48 Recent 1.03\n",
      "Episode 6\tAverage Score: 0.71             Episode 6\tAverage Score: 0.71 Recent 1.16\n",
      "Episode 8\tAverage Score: 0.91             Episode 8\tAverage Score: 0.91 Recent 1.72\n",
      "Episode 10\tAverage Score: 1.17            Episode 10\tAverage Score: 1.17 Recent 2.62\n",
      "Episode 12\tAverage Score: 1.47            Episode 12\tAverage Score: 1.47 Recent 3.32\n",
      "Episode 14\tAverage Score: 1.93            Episode 14\tAverage Score: 1.93 Recent 5.45\n",
      "Episode 16\tAverage Score: 2.52            Episode 16\tAverage Score: 2.52 Recent 8.56\n",
      "Episode 18\tAverage Score: 3.28            Episode 18\tAverage Score: 3.28 Recent 9.99\n",
      "Episode 20\tAverage Score: 4.27            Episode 20\tAverage Score: 4.27 Recent 14.70\n",
      "Episode 22\tAverage Score: 5.37            Episode 22\tAverage Score: 5.37 Recent 16.85\n",
      "Episode 24\tAverage Score: 6.66            Episode 24\tAverage Score: 6.66 Recent 22.61\n",
      "Episode 26\tAverage Score: 8.16            Episode 26\tAverage Score: 8.16 Recent 28.50\n",
      "                                          Episode 27\tAverage Score: 8.96 Recent 30.42DONE! -->  30.419999320060025 27\n",
      "                                          Episode 28\tAverage Score: 9.87 Recent 35.40DONE! -->  35.39649920882668 28\n",
      "Episode 28\tAverage Score: 9.87\n",
      "                                          Episode 29\tAverage Score: 10.75 Recent 36.32DONE! -->  36.32399918809549 29\n",
      "                                          Episode 30\tAverage Score: 11.62 Recent 37.68DONE! -->  37.67799915783118 30\n",
      "Episode 30\tAverage Score: 11.62\n",
      "                                          Episode 31\tAverage Score: 12.44 Recent 37.87DONE! -->  37.86849915357323 31\n",
      "                                          Episode 32\tAverage Score: 13.22 Recent 38.12DONE! -->  38.122999147884784 32\n",
      "Episode 32\tAverage Score: 13.22\n",
      "                                          Episode 33\tAverage Score: 13.96 Recent 38.48DONE! -->  38.4769991399722 33\n",
      "                                          Episode 34\tAverage Score: 14.68 Recent 39.06DONE! -->  39.0634991268629 34\n",
      "Episode 34\tAverage Score: 14.68\n",
      "                                          Episode 35\tAverage Score: 15.34 Recent 38.53DONE! -->  38.534999138675815 35\n",
      "                                          Episode 36\tAverage Score: 15.99 Recent 39.47DONE! -->  39.46799911782146 36\n",
      "Episode 36\tAverage Score: 15.99\n",
      "                                          Episode 37\tAverage Score: 16.61 Recent 39.29DONE! -->  39.28599912188954 37\n",
      "                                          Episode 38\tAverage Score: 17.19 Recent 39.45DONE! -->  39.447999118268484 38\n",
      "Episode 38\tAverage Score: 17.19\n",
      "                                          Episode 39\tAverage Score: 17.74 Recent 39.27DONE! -->  39.269499122258274 39\n",
      "                                          Episode 40\tAverage Score: 18.27 Recent 39.47DONE! -->  39.47349911769852 40\n",
      "Episode 40\tAverage Score: 18.27\n",
      "                                          Episode 41\tAverage Score: 18.77 Recent 38.94DONE! -->  38.9409991296009 41\n",
      "                                          Episode 42\tAverage Score: 19.25 Recent 39.46DONE! -->  39.45549911810085 42\n",
      "Episode 42\tAverage Score: 19.25\n",
      "                                          Episode 43\tAverage Score: 19.71 Recent 39.50DONE! -->  39.50249911705032 43\n",
      "                                          Episode 44\tAverage Score: 20.15 Recent 39.47DONE! -->  39.4729991177097 44\n",
      "Episode 44\tAverage Score: 20.15\n",
      "                                          Episode 45\tAverage Score: 20.57 Recent 39.51DONE! -->  39.50799911692739 45\n",
      "                                          Episode 46\tAverage Score: 20.97 Recent 39.43DONE! -->  39.428499118704345 46\n",
      "Episode 46\tAverage Score: 20.97\n",
      "                                          Episode 47\tAverage Score: 21.35 Recent 39.47DONE! -->  39.469499117787926 47\n",
      "                                          Episode 48\tAverage Score: 21.72 Recent 39.36DONE! -->  39.35699912030252 48\n",
      "Episode 48\tAverage Score: 21.72\n",
      "                                          Episode 49\tAverage Score: 22.08 Recent 39.40DONE! -->  39.400499119330206 49\n",
      "                                          Episode 50\tAverage Score: 22.42 Recent 39.40DONE! -->  39.40449911924079 50\n",
      "Episode 50\tAverage Score: 22.42\n",
      "                                          Episode 51\tAverage Score: 22.74 Recent 39.26DONE! -->  39.258999122493044 51\n",
      "                                          Episode 52\tAverage Score: 23.05 Recent 39.40DONE! -->  39.403999119251964 52\n",
      "Episode 52\tAverage Score: 23.05\n",
      "                                          Episode 53\tAverage Score: 23.36 Recent 39.44DONE! -->  39.43949911845848 53\n",
      "                                          Episode 54\tAverage Score: 23.65 Recent 39.36DONE! -->  39.36099912021315 54\n",
      "Episode 54\tAverage Score: 23.65\n",
      "                                          Episode 55\tAverage Score: 23.93 Recent 39.49DONE! -->  39.49049911731854 55\n",
      "                                          Episode 56\tAverage Score: 24.20 Recent 39.44DONE! -->  39.443999118357894 56\n",
      "Episode 56\tAverage Score: 24.20\n",
      "                                          Episode 57\tAverage Score: 24.46 Recent 39.36DONE! -->  39.36299912016839 57\n",
      "                                          Episode 58\tAverage Score: 24.72 Recent 39.48DONE! -->  39.47749911760911 58\n",
      "Episode 58\tAverage Score: 24.72\n",
      "                                          Episode 59\tAverage Score: 24.96 Recent 39.41DONE! -->  39.41249911906196 59\n",
      "                                          Episode 60\tAverage Score: 25.20 Recent 39.46DONE! -->  39.46399911791086 60\n",
      "Episode 60\tAverage Score: 25.20\n",
      "                                          Episode 61\tAverage Score: 25.43 Recent 39.51DONE! -->  39.50849911691621 61\n",
      "                                          Episode 62\tAverage Score: 25.66 Recent 39.51DONE! -->  39.51049911687151 62\n",
      "Episode 62\tAverage Score: 25.66\n",
      "                                          Episode 63\tAverage Score: 25.87 Recent 39.52DONE! -->  39.521499116625634 63\n",
      "                                          Episode 64\tAverage Score: 26.08 Recent 39.48DONE! -->  39.480999117530885 64\n",
      "Episode 64\tAverage Score: 26.08\n",
      "                                          Episode 65\tAverage Score: 26.28 Recent 39.45DONE! -->  39.44699911829084 65\n",
      "                                          Episode 66\tAverage Score: 26.48 Recent 39.48DONE! -->  39.48199911750853 66\n",
      "Episode 66\tAverage Score: 26.48\n",
      "                                          Episode 67\tAverage Score: 26.67 Recent 39.50DONE! -->  39.49899911712855 67\n",
      "                                          Episode 68\tAverage Score: 26.86 Recent 39.51DONE! -->  39.50799911692739 68\n",
      "Episode 68\tAverage Score: 26.86\n",
      "                                          Episode 69\tAverage Score: 27.04 Recent 39.42DONE! -->  39.423499118816196 69\n",
      "                                          Episode 70\tAverage Score: 27.21 Recent 39.53DONE! -->  39.53249911637978 70\n",
      "Episode 70\tAverage Score: 27.21\n",
      "                                          Episode 71\tAverage Score: 27.38 Recent 39.46DONE! -->  39.45549911810092 71\n",
      "                                          Episode 72\tAverage Score: 27.55 Recent 39.52DONE! -->  39.5244991165586 72\n",
      "Episode 72\tAverage Score: 27.55\n",
      "                                          Episode 73\tAverage Score: 27.71 Recent 39.43DONE! -->  39.432999118603874 73\n",
      "                                          Episode 74\tAverage Score: 27.87 Recent 39.31DONE! -->  39.31149912131965 74\n",
      "Episode 74\tAverage Score: 27.87\n",
      "                                          Episode 75\tAverage Score: 28.02 Recent 39.39DONE! -->  39.394499119464385 75\n",
      "                                          Episode 76\tAverage Score: 28.17 Recent 39.42DONE! -->  39.42399911880497 76\n",
      "Episode 76\tAverage Score: 28.17\n",
      "                                          Episode 77\tAverage Score: 28.31 Recent 39.45DONE! -->  39.448499118257324 77\n",
      "                                          Episode 78\tAverage Score: 28.45 Recent 39.30DONE! -->  39.29949912158788 78\n",
      "Episode 78\tAverage Score: 28.45\n",
      "                                          Episode 79\tAverage Score: 28.59 Recent 39.38DONE! -->  39.382499119732564 79\n",
      "                                          Episode 80\tAverage Score: 28.72 Recent 39.45DONE! -->  39.45249911816798 80\n",
      "Episode 80\tAverage Score: 28.72\n",
      "                                          Episode 81\tAverage Score: 28.85 Recent 39.39DONE! -->  39.38849911959848 81\n",
      "                                          Episode 82\tAverage Score: 28.98 Recent 39.43DONE! -->  39.42949911868207 82\n",
      "Episode 82\tAverage Score: 28.98\n",
      "                                          Episode 83\tAverage Score: 29.10 Recent 38.96DONE! -->  38.9554991292769 83\n",
      "                                          Episode 84\tAverage Score: 29.21 Recent 38.83DONE! -->  38.8259991321713 84\n",
      "Episode 84\tAverage Score: 29.21\n",
      "                                          Episode 85\tAverage Score: 29.33 Recent 39.08DONE! -->  39.08199912644922 85\n",
      "                                          Episode 86\tAverage Score: 29.44 Recent 39.30DONE! -->  39.29649912165487 86\n",
      "Episode 86\tAverage Score: 29.44\n",
      "                                          Episode 87\tAverage Score: 29.55 Recent 39.25DONE! -->  39.24949912270534 87\n",
      "                                          Episode 88\tAverage Score: 29.66 Recent 39.22DONE! -->  39.224499123264195 88\n",
      "Episode 88\tAverage Score: 29.66\n",
      "                                          Episode 89\tAverage Score: 29.77 Recent 39.16DONE! -->  39.15849912473947 89\n",
      "                                          Episode 90\tAverage Score: 29.86 Recent 38.73DONE! -->  38.726999134384364 90\n",
      "Episode 90\tAverage Score: 29.86\n",
      "                                          Episode 91\tAverage Score: 29.96 Recent 38.95DONE! -->  38.95199912935515 91\n",
      "                                          Episode 92\tAverage Score: 30.06 Recent 39.01DONE! -->  39.01199912801397 92\n",
      "Episode 92\tAverage Score: 30.06\n"
     ]
    }
   ],
   "source": [
    "def save_model():\n",
    "    for i, l in enumerate(agent.ac_explorer.actor.layers):\n",
    "        torch.save(l.state_dict(), 'checkpoint_explorer_nes_layer_%i.pth'%i)\n",
    "    for i, l in enumerate(agent.ac_target.actor.layers):\n",
    "        torch.save(l.state_dict(), 'checkpoint_target_nes_layer_%i.pth'%i)\n",
    "        \n",
    "    torch.save(agent.ac_explorer.state_dict(), 'checkpoint_explorer.pth')\n",
    "    torch.save(agent.ac_target.state_dict(), 'checkpoint_target.pth')\n",
    "\n",
    "def ddpg(max_t=1001, print_every=2):\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores_deque.append(0)\n",
    "    scores = []\n",
    "    while np.mean(scores_deque) < 30.:\n",
    "        n_states = deque(maxlen=N_STEP)\n",
    "        n_actions = deque(maxlen=N_STEP)\n",
    "        n_rewards = deque(maxlen=N_STEP)\n",
    "        \n",
    "        td_lamda = lambda: map(\n",
    "                lambda i: sum(map(lambda j: n_rewards[j][i] * (GAMMA ** j), range(N_STEP))),\n",
    "                range(len(n_rewards[0])))\n",
    "        \n",
    "        score = 0\n",
    "        einfo = ENV.reset()[BRAIN_NAME]\n",
    "        for t in range(max_t):\n",
    "            states = einfo.vector_observations.copy()\n",
    "            actions = agent.explore(states)\n",
    "            einfo = ENV.step(actions.reshape(-1))[BRAIN_NAME]\n",
    "            \n",
    "            if t > N_STEP:\n",
    "                for s, a, r, n in zip(                \n",
    "                    n_states[0],\n",
    "                    n_actions[0].reshape(len(states), -1),\n",
    "                    td_lamda(),\n",
    "                    states): agent.step(s, a, r, n, t)\n",
    "                \n",
    "            n_states.append(states)\n",
    "            n_actions.append(actions)\n",
    "            n_rewards.append(einfo.rewards)\n",
    "            \n",
    "            score += np.mean(einfo.rewards)\n",
    "            if sum(einfo.local_done):\n",
    "                break \n",
    "                \n",
    "            print('\\r[{}] steps :: {} {:.2f}'.format(len(scores), t, score), end=\"\")\n",
    "\n",
    "        scores_deque.append(score)\n",
    "        scores.append(score)\n",
    "        \n",
    "        print('\\r{}Episode {}\\tAverage Score: {:.2f} Recent {:.2f}'.format(\n",
    "            (' ' * 42), len(scores), np.mean(scores_deque), score), end=\"\")\n",
    "        \n",
    "        if score > 30.:\n",
    "            print(\"DONE! --> \", score, len(scores))\n",
    "        \n",
    "        save_model()\n",
    "        if len(scores) % print_every == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(len(scores), np.mean(scores_deque)))\n",
    "            \n",
    "    return scores\n",
    "\n",
    "scores = ddpg()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Plot reward ( TODO: loss-es )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores)+1), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.Reload trained agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "agent.ac_explorer.load_state_dict(torch.load('checkpoint_explorer.pth'))\n",
    "agent.ac_target.load_state_dict(torch.load('checkpoint_target.pth'))\n",
    "\n",
    "for i, l in enumerate(agent.ac_explorer.actor.layers):\n",
    "    l.load_state_dict(torch.load('checkpoint_explorer_nes_layer_%i.pth'%i))\n",
    "for i, l in enumerate(agent.ac_target.actor.layers):\n",
    "    l.load_state_dict(torch.load('checkpoint_target_nes_layer_%i.pth'%i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Test visuals - EXPLORER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "einfo = ENV.reset()[BRAIN_NAME]\n",
    "while not sum(einfo.local_done):\n",
    "    states = einfo.vector_observations.copy()\n",
    "    actions = agent.explore(states)\n",
    "    einfo = ENV.step(actions.reshape(-1))[BRAIN_NAME]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Test visuals - TARGET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "einfo = ENV.reset()[BRAIN_NAME]\n",
    "while not sum(einfo.local_done):\n",
    "    states = einfo.vector_observations.copy()\n",
    "    actions = agent.exploit(states)\n",
    "    einfo = ENV.step(actions.reshape(-1))[BRAIN_NAME]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Close environment, we are done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ENV.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. TODO\n",
    "\n",
    "Port it to my wheeler framework; this should server as a benchmark ~ faster, easier to play with\n",
    "- on framework we can test : \n",
    "  - HER - https://arxiv.org/abs/1707.01495\n",
    "  - cooperation PPO + DDPG\n",
    "  - multi-task\n",
    "  - prio buffer by curiosity\n",
    "  - detached ACtor + Critic ( local from target networks )"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Anaconda3]",
   "language": "python",
   "name": "Python [Anaconda3]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
